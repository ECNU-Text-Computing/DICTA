inputs: torch.Size([5, 4, 16]) 
 tensor([[[9., 6., 7., 4., 7., 4., 3., 3., 5., 5., 8., 4., 2., 8., 5., 7.],
         [6., 4., 8., 9., 2., 2., 1., 8., 1., 9., 3., 1., 7., 4., 1., 9.],
         [4., 2., 6., 8., 1., 5., 6., 1., 1., 3., 1., 6., 8., 8., 8., 3.],
         [6., 6., 9., 7., 7., 8., 2., 3., 5., 6., 6., 6., 7., 2., 2., 5.]],

        [[6., 1., 3., 8., 4., 5., 5., 1., 6., 9., 9., 9., 2., 5., 4., 7.],
         [5., 2., 3., 4., 1., 1., 5., 8., 5., 9., 7., 2., 8., 4., 3., 4.],
         [7., 2., 2., 5., 9., 1., 7., 9., 4., 3., 3., 6., 1., 4., 7., 7.],
         [1., 4., 5., 4., 6., 7., 9., 8., 9., 8., 2., 6., 4., 6., 2., 5.]],

        [[6., 1., 5., 6., 3., 3., 1., 8., 1., 5., 1., 9., 7., 4., 8., 5.],
         [4., 2., 2., 2., 1., 3., 4., 1., 8., 6., 8., 4., 5., 2., 2., 1.],
         [7., 3., 8., 7., 2., 8., 6., 8., 8., 7., 2., 8., 5., 5., 6., 1.],
         [9., 1., 5., 9., 4., 2., 3., 9., 7., 7., 4., 9., 7., 7., 4., 6.]],

        [[5., 9., 3., 7., 1., 3., 1., 3., 6., 2., 5., 1., 4., 3., 6., 1.],
         [7., 2., 7., 6., 7., 4., 8., 1., 3., 6., 4., 4., 9., 3., 5., 9.],
         [4., 7., 7., 1., 6., 4., 7., 7., 5., 2., 8., 1., 8., 2., 4., 5.],
         [1., 1., 7., 2., 7., 1., 6., 1., 4., 6., 2., 6., 8., 5., 2., 7.]],

        [[6., 1., 9., 2., 3., 8., 6., 6., 3., 4., 4., 9., 7., 9., 5., 7.],
         [9., 1., 8., 7., 7., 2., 6., 7., 1., 3., 4., 4., 8., 6., 5., 6.],
         [1., 2., 5., 2., 8., 4., 1., 6., 6., 1., 4., 2., 2., 8., 4., 1.],
         [7., 1., 6., 5., 6., 1., 9., 7., 8., 9., 3., 8., 9., 9., 5., 3.]]])
================================================================================
进入多头注意力计算:
	 多头num_heads = 2, d_model=16, d_k = d_v = d_model/num_heads=8
	 query的shape([tgt_len, batch_size, input_dim]):torch.Size([5, 4, 16])
	  W_q 的shape([input_dim, kdim * num_heads]):torch.Size([16, 16])
	   Q  的shape([tgt_len, batch_size, kdim * num_heads]):torch.Size([5, 4, 16])
	----------------------------------------------------------------------
	  key 的shape([src_len,batch_size, input_dim]):torch.Size([5, 4, 16])
	  W_k 的shape([input_dim, kdim * num_heads]):torch.Size([16, 16])
	   K  的shape([src_len,batch_size, kdim * num_heads]):torch.Size([5, 4, 16])
	----------------------------------------------------------------------
	 value的shape([src_len,batch_size, input_dim]):torch.Size([5, 4, 16])
	  W_v 的shape([input_dim, vdim * num_heads]):torch.Size([16, 16])
	   V  的shape([src_len,batch_size, vdim * num_heads]):torch.Size([5, 4, 16])
	----------------------------------------------------------------------
	 ***** 注意，这里的W_q, W_k, W_v是多个head同时进行计算的。 因此，Q,K,V分别也是包含了多个head的q,k,v堆叠起来的结果 *****
	 多头注意力中,多头计算结束后的形状（堆叠）为([tgt_len,batch_size,num_heads*kdim])torch.Size([5, 4, 16])
	 多头计算结束后，再进行线性变换时的权重W_o的形状为([num_heads*vdim, num_heads*vdim  ])torch.Size([16, 16])
	 多头线性变化后的形状为([tgt_len,batch_size,input_dim]) torch.Size([5, 4, 16])
outputs: torch.Size([5, 4, 16]) 
 tensor([[[ 24.3429,  14.1572,  11.9690,   4.6417,  -0.4891,  -2.0127,  -3.5288,
            0.8721,   6.2031,  10.7582,  15.3540,   9.6693,  13.4769,  21.2375,
            7.9772,  11.1384],
         [ 13.2291,   8.7338,  -3.0960,  15.5572,  -2.5692,  -6.3882,  -7.2198,
           -2.8469,   4.9615,  10.9501,   1.7708,   1.8864,   2.0467,  13.2755,
            0.5752,   3.1473],
         [ 17.0091,   7.1891,  15.9908,   5.0413,  -4.0734,   4.6454,   3.6216,
            5.8271,   0.1378,   8.9874,   8.0657,   8.0145,  20.5341,  14.6274,
           10.7100,   9.2224],
         [ 27.4369,  16.1389,  11.6630,   6.6602,   0.9584,   0.3594,  -8.9705,
           -0.9160,   0.5222,  16.2568,  15.6258,  15.4244,  15.4489,  15.9589,
            3.5076,   9.3766]],

        [[ 21.8053,   6.6106,   6.3745,   8.2673,   1.1341,  -0.6869,  -3.3474,
           -1.1472,   5.3226,  17.6804,  14.6709,  17.5893,  11.1342,  17.1534,
            7.1142,   8.7962],
         [ 12.3556,   6.7558,  -5.4212,  10.4785,  -4.1520,  -8.1677,  -2.2093,
           -2.6253,   8.6844,   8.5940,   8.2611,   3.7542,   4.7427,  12.1808,
            2.7206,   0.1119],
         [ 18.8807,   9.2447,  11.1446,   3.3647,   3.7682,  -2.1766,  -0.2652,
            9.3157,   0.1495,   8.8680,   9.6001,  12.7347,  12.4947,  13.4687,
           12.1240,   7.2009],
         [ 11.9676,  10.7115,  -3.8516,  10.1682,   0.3849,  -2.7755,  -0.1903,
           -3.6079,  11.7872,   9.9885,   4.1386,   9.1742,   1.1574,  16.3838,
            1.1200,   1.8550]],

        [[ 21.8042,   6.6255,   8.3814,   6.2700,   0.1080,  -2.6909,  -7.3376,
            5.8502,   0.3307,  13.6663,   6.6824,  17.5745,  16.1477,  16.1612,
           11.1139,   6.8091],
         [ 11.7034,   6.8812,  -9.7249,   7.2420,  -2.2418,  -4.5739,  -4.1317,
          -10.4972,  11.2476,  10.2745,   7.1580,   8.1744,   1.3446,  12.7837,
            3.3120,  -4.4485],
         [ 21.4861,  13.0655,  10.6572,   5.7500,  -1.1186,   5.6381,  -6.0985,
            3.0302,   8.7547,  11.6112,   6.4133,  18.5917,   9.9862,  16.8092,
            7.1874,   2.2181],
         [ 28.1213,  12.0810,   9.4037,   7.2205,  -0.9973,  -4.1482,  -7.5568,
            5.9147,   1.2399,  16.9328,  12.2893,  20.4797,  16.0095,  20.0642,
            6.0759,  10.4312]],

        [[ 17.2761,  19.2834,   7.8697,   8.7165,  -8.1739,   1.5544,  -7.2273,
           -0.6532,  11.8928,  -0.4495,  11.8770,   0.8838,  11.9988,  13.0017,
            5.1821,   5.4971],
         [ 28.1782,  12.1847,   8.1791,   3.6002,   2.1705,   1.7258,  -4.0541,
           -0.3502,   2.3585,  15.3158,   8.1499,  12.4521,  11.0481,  14.3642,
            5.5042,  12.8203],
         [ 15.2998,  13.6837,  17.6849,  -1.7562,   1.8554,   2.2372,   0.4478,
            8.8351,  -0.4781,   8.4811,  14.4908,   7.3472,  19.3727,   9.6539,
            8.6542,   5.5346],
         [ 20.4995,  11.7795,  11.3535,   0.3282,   1.8832,  -5.1775,  -4.4317,
           -1.9284,  -1.5217,  15.9870,  10.5616,  16.9742,  16.9988,  17.9977,
            3.9566,  11.5796]],

        [[ 21.8054,   6.6115,  12.3750,   2.2675,   0.1325,   2.3127,  -2.3468,
            3.8527,   2.3230,  12.6797,   9.6716,  17.5886,  16.1351,  21.1539,
            8.1142,   8.7969],
         [ 28.1548,  10.0431,  11.9486,   5.3431,   1.6514,  -1.0567,  -3.7852,
            5.9883,   0.4495,   8.8406,  10.1422,  11.8277,  11.6101,  14.9753,
            5.9461,  10.7683],
         [ 12.7933,   9.3003,  11.1914,   0.7569,   4.7425,   2.0495,  -6.3499,
            3.5434,   2.7430,   5.2344,   9.8687,   7.7198,   8.3663,  14.3933,
            5.3029,   2.1490],
         [ 25.3159,  11.9384,   9.5195,   3.6467,   1.3385,  -5.1602,  -1.2852,
            2.9906,   2.5929,  18.2113,  10.8384,  19.2530,  16.8773,  21.3997,
            6.4474,   7.1887]]], grad_fn=<AddBackward0>)
