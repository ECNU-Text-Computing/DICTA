2023-05-14 01:23:14,512 root         INFO     Namespace(data_path='dblp', expt_dir='./experiment/baseline_rnn', log_level='info', model='rnn')
2023-05-14 01:23:14,513 sentence_transformers.SentenceTransformer INFO     Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2023-05-14 01:23:14,834 sentence_transformers.SentenceTransformer INFO     Use pytorch device: cuda
2023-05-14 01:23:18,102 seq2seq.trainer.supervised_trainer INFO     Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
), Scheduler: None
2023-05-14 01:23:36,248 seq2seq.trainer.supervised_trainer INFO     Progress: 3%, Train RMSE Loss: 0.0871
2023-05-14 01:23:44,780 seq2seq.trainer.supervised_trainer INFO     Progress: 4%, Train RMSE Loss: 0.0402
2023-05-14 01:23:53,113 seq2seq.trainer.supervised_trainer INFO     Progress: 6%, Train RMSE Loss: 0.0389
2023-05-14 01:24:01,460 seq2seq.trainer.supervised_trainer INFO     Progress: 8%, Train RMSE Loss: 0.0379
2023-05-14 01:24:10,275 seq2seq.trainer.supervised_trainer INFO     Progress: 9%, Train RMSE Loss: 0.0367
2023-05-14 01:24:28,043 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 1: Train RMSE Loss: 0.0401, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:24:36,675 seq2seq.trainer.supervised_trainer INFO     Progress: 11%, Train RMSE Loss: 0.0361
2023-05-14 01:24:45,255 seq2seq.trainer.supervised_trainer INFO     Progress: 13%, Train RMSE Loss: 0.0360
2023-05-14 01:24:53,441 seq2seq.trainer.supervised_trainer INFO     Progress: 14%, Train RMSE Loss: 0.0357
2023-05-14 01:25:01,971 seq2seq.trainer.supervised_trainer INFO     Progress: 16%, Train RMSE Loss: 0.0364
2023-05-14 01:25:11,204 seq2seq.trainer.supervised_trainer INFO     Progress: 18%, Train RMSE Loss: 0.0341
2023-05-14 01:25:20,019 seq2seq.trainer.supervised_trainer INFO     Progress: 19%, Train RMSE Loss: 0.0344
2023-05-14 01:25:36,719 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 2: Train RMSE Loss: 0.0355, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:25:44,084 seq2seq.trainer.supervised_trainer INFO     Progress: 21%, Train RMSE Loss: 0.0334
2023-05-14 01:25:52,613 seq2seq.trainer.supervised_trainer INFO     Progress: 23%, Train RMSE Loss: 0.0363
2023-05-14 01:26:01,301 seq2seq.trainer.supervised_trainer INFO     Progress: 24%, Train RMSE Loss: 0.0327
2023-05-14 01:26:09,984 seq2seq.trainer.supervised_trainer INFO     Progress: 26%, Train RMSE Loss: 0.0320
2023-05-14 01:26:19,009 seq2seq.trainer.supervised_trainer INFO     Progress: 27%, Train RMSE Loss: 0.0317
2023-05-14 01:26:27,764 seq2seq.trainer.supervised_trainer INFO     Progress: 29%, Train RMSE Loss: 0.0330
2023-05-14 01:26:46,092 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 3: Train RMSE Loss: 0.0331, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:26:52,938 seq2seq.trainer.supervised_trainer INFO     Progress: 31%, Train RMSE Loss: 0.0306
2023-05-14 01:27:01,933 seq2seq.trainer.supervised_trainer INFO     Progress: 32%, Train RMSE Loss: 0.0316
2023-05-14 01:27:10,590 seq2seq.trainer.supervised_trainer INFO     Progress: 34%, Train RMSE Loss: 0.0331
2023-05-14 01:27:19,245 seq2seq.trainer.supervised_trainer INFO     Progress: 36%, Train RMSE Loss: 0.0313
2023-05-14 01:27:27,900 seq2seq.trainer.supervised_trainer INFO     Progress: 37%, Train RMSE Loss: 0.0310
2023-05-14 01:27:36,640 seq2seq.trainer.supervised_trainer INFO     Progress: 39%, Train RMSE Loss: 0.0319
2023-05-14 01:27:55,621 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 4: Train RMSE Loss: 0.0315, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:28:01,881 seq2seq.trainer.supervised_trainer INFO     Progress: 41%, Train RMSE Loss: 0.0312
2023-05-14 01:28:11,120 seq2seq.trainer.supervised_trainer INFO     Progress: 42%, Train RMSE Loss: 0.0315
2023-05-14 01:28:20,048 seq2seq.trainer.supervised_trainer INFO     Progress: 44%, Train RMSE Loss: 0.0317
2023-05-14 01:28:29,146 seq2seq.trainer.supervised_trainer INFO     Progress: 46%, Train RMSE Loss: 0.0300
2023-05-14 01:28:37,561 seq2seq.trainer.supervised_trainer INFO     Progress: 47%, Train RMSE Loss: 0.0320
2023-05-14 01:28:45,850 seq2seq.trainer.supervised_trainer INFO     Progress: 49%, Train RMSE Loss: 0.0309
2023-05-14 01:29:05,718 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 5: Train RMSE Loss: 0.0310, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:29:11,301 seq2seq.trainer.supervised_trainer INFO     Progress: 51%, Train RMSE Loss: 0.0286
2023-05-14 01:29:20,324 seq2seq.trainer.supervised_trainer INFO     Progress: 52%, Train RMSE Loss: 0.0289
2023-05-14 01:29:29,485 seq2seq.trainer.supervised_trainer INFO     Progress: 54%, Train RMSE Loss: 0.0313
2023-05-14 01:29:37,924 seq2seq.trainer.supervised_trainer INFO     Progress: 55%, Train RMSE Loss: 0.0310
2023-05-14 01:29:46,489 seq2seq.trainer.supervised_trainer INFO     Progress: 57%, Train RMSE Loss: 0.0307
2023-05-14 01:29:55,489 seq2seq.trainer.supervised_trainer INFO     Progress: 59%, Train RMSE Loss: 0.0297
2023-05-14 01:30:16,283 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 6: Train RMSE Loss: 0.0302, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:30:20,896 seq2seq.trainer.supervised_trainer INFO     Progress: 60%, Train RMSE Loss: 0.0295
2023-05-14 01:30:29,432 seq2seq.trainer.supervised_trainer INFO     Progress: 62%, Train RMSE Loss: 0.0305
2023-05-14 01:30:37,694 seq2seq.trainer.supervised_trainer INFO     Progress: 64%, Train RMSE Loss: 0.0298
2023-05-14 01:30:46,100 seq2seq.trainer.supervised_trainer INFO     Progress: 65%, Train RMSE Loss: 0.0308
2023-05-14 01:30:55,158 seq2seq.trainer.supervised_trainer INFO     Progress: 67%, Train RMSE Loss: 0.0313
2023-05-14 01:31:03,807 seq2seq.trainer.supervised_trainer INFO     Progress: 69%, Train RMSE Loss: 0.0295
2023-05-14 01:31:25,463 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 7: Train RMSE Loss: 0.0301, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:31:29,397 seq2seq.trainer.supervised_trainer INFO     Progress: 70%, Train RMSE Loss: 0.0295
2023-05-14 01:31:37,528 seq2seq.trainer.supervised_trainer INFO     Progress: 72%, Train RMSE Loss: 0.0284
2023-05-14 01:31:46,337 seq2seq.trainer.supervised_trainer INFO     Progress: 74%, Train RMSE Loss: 0.0314
2023-05-14 01:31:54,939 seq2seq.trainer.supervised_trainer INFO     Progress: 75%, Train RMSE Loss: 0.0302
2023-05-14 01:32:03,523 seq2seq.trainer.supervised_trainer INFO     Progress: 77%, Train RMSE Loss: 0.0304
2023-05-14 01:32:12,132 seq2seq.trainer.supervised_trainer INFO     Progress: 78%, Train RMSE Loss: 0.0284
2023-05-14 01:32:32,885 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 8: Train RMSE Loss: 0.0295, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:32:36,055 seq2seq.trainer.supervised_trainer INFO     Progress: 80%, Train RMSE Loss: 0.0269
2023-05-14 01:32:44,026 seq2seq.trainer.supervised_trainer INFO     Progress: 82%, Train RMSE Loss: 0.0289
2023-05-14 01:32:52,145 seq2seq.trainer.supervised_trainer INFO     Progress: 83%, Train RMSE Loss: 0.0305
2023-05-14 01:33:00,138 seq2seq.trainer.supervised_trainer INFO     Progress: 85%, Train RMSE Loss: 0.0293
2023-05-14 01:33:07,525 seq2seq.trainer.supervised_trainer INFO     Progress: 87%, Train RMSE Loss: 0.0313
2023-05-14 01:33:14,792 seq2seq.trainer.supervised_trainer INFO     Progress: 88%, Train RMSE Loss: 0.0306
2023-05-14 01:33:32,920 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 9: Train RMSE Loss: 0.0298, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:33:35,294 seq2seq.trainer.supervised_trainer INFO     Progress: 90%, Train RMSE Loss: 0.0290
2023-05-14 01:33:42,384 seq2seq.trainer.supervised_trainer INFO     Progress: 92%, Train RMSE Loss: 0.0275
2023-05-14 01:33:48,979 seq2seq.trainer.supervised_trainer INFO     Progress: 93%, Train RMSE Loss: 0.0304
2023-05-14 01:33:55,597 seq2seq.trainer.supervised_trainer INFO     Progress: 95%, Train RMSE Loss: 0.0303
2023-05-14 01:34:02,147 seq2seq.trainer.supervised_trainer INFO     Progress: 97%, Train RMSE Loss: 0.0286
2023-05-14 01:34:09,073 seq2seq.trainer.supervised_trainer INFO     Progress: 98%, Train RMSE Loss: 0.0299
2023-05-14 01:34:46,654 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 10: Train RMSE Loss: 0.0293, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
Train RMSE Loss from Evaluator: 0.1312
------Hyper-parameters------
model_name: rnn, batch_size: 64, hidden_size: 32, learning_rate: 0.001, num_epochs: 10, num_layers: 3, dropout_rate: 0.1, teacher_forcing_ratio: 0.5
input_size: 128, kernel_size: 3, num_head: 2, use_adamw: False
use_sbert = False, use_sbert_seq = False, in_len: 4, out_len: 5
----------------------------
--------------Best Model:--------------
Epoch 5, Train RMSE Loss: 0.1312, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
---------------------------------------
