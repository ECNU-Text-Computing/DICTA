2023-05-14 01:53:03,675 root         INFO     Namespace(data_path='dblp', expt_dir='./experiment/no_att_abs', log_level='info', model='transformer')
2023-05-14 01:53:03,675 sentence_transformers.SentenceTransformer INFO     Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2023-05-14 01:53:03,980 sentence_transformers.SentenceTransformer INFO     Use pytorch device: cuda
2023-05-14 01:53:06,234 seq2seq.trainer.supervised_trainer INFO     Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
), Scheduler: None
2023-05-14 01:53:47,794 seq2seq.trainer.supervised_trainer INFO     Progress: 3%, Train RMSE Loss: 0.1585
2023-05-14 01:54:07,322 seq2seq.trainer.supervised_trainer INFO     Progress: 4%, Train RMSE Loss: 0.0767
2023-05-14 01:54:27,906 seq2seq.trainer.supervised_trainer INFO     Progress: 6%, Train RMSE Loss: 0.0770
2023-05-14 01:54:47,237 seq2seq.trainer.supervised_trainer INFO     Progress: 8%, Train RMSE Loss: 0.0769
2023-05-14 01:55:07,370 seq2seq.trainer.supervised_trainer INFO     Progress: 9%, Train RMSE Loss: 0.0763
2023-05-14 01:55:32,667 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 1: Train RMSE Loss: 0.0776, Dev RMSE Loss: 0.0754, Test RMSE Loss: 0.0760
2023-05-14 01:55:51,984 seq2seq.trainer.supervised_trainer INFO     Progress: 11%, Train RMSE Loss: 0.0763
2023-05-14 01:56:13,038 seq2seq.trainer.supervised_trainer INFO     Progress: 13%, Train RMSE Loss: 0.0766
2023-05-14 01:56:34,249 seq2seq.trainer.supervised_trainer INFO     Progress: 14%, Train RMSE Loss: 0.0757
2023-05-14 01:56:55,999 seq2seq.trainer.supervised_trainer INFO     Progress: 16%, Train RMSE Loss: 0.0765
2023-05-14 01:57:17,694 seq2seq.trainer.supervised_trainer INFO     Progress: 18%, Train RMSE Loss: 0.0760
2023-05-14 01:57:39,267 seq2seq.trainer.supervised_trainer INFO     Progress: 19%, Train RMSE Loss: 0.0759
2023-05-14 01:58:08,241 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 2: Train RMSE Loss: 0.0762, Dev RMSE Loss: 0.0745, Test RMSE Loss: 0.0751
2023-05-14 01:58:26,090 seq2seq.trainer.supervised_trainer INFO     Progress: 21%, Train RMSE Loss: 0.0758
2023-05-14 01:58:47,599 seq2seq.trainer.supervised_trainer INFO     Progress: 23%, Train RMSE Loss: 0.0760
2023-05-14 01:59:08,657 seq2seq.trainer.supervised_trainer INFO     Progress: 24%, Train RMSE Loss: 0.0755
2023-05-14 01:59:29,304 seq2seq.trainer.supervised_trainer INFO     Progress: 26%, Train RMSE Loss: 0.0758
2023-05-14 01:59:49,767 seq2seq.trainer.supervised_trainer INFO     Progress: 27%, Train RMSE Loss: 0.0757
2023-05-14 02:00:11,155 seq2seq.trainer.supervised_trainer INFO     Progress: 29%, Train RMSE Loss: 0.0760
2023-05-14 02:00:41,621 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 3: Train RMSE Loss: 0.0758, Dev RMSE Loss: 0.0751, Test RMSE Loss: 0.0757
2023-05-14 02:00:58,295 seq2seq.trainer.supervised_trainer INFO     Progress: 31%, Train RMSE Loss: 0.0756
2023-05-14 02:01:19,571 seq2seq.trainer.supervised_trainer INFO     Progress: 32%, Train RMSE Loss: 0.0756
2023-05-14 02:01:40,564 seq2seq.trainer.supervised_trainer INFO     Progress: 34%, Train RMSE Loss: 0.0754
2023-05-14 02:02:02,033 seq2seq.trainer.supervised_trainer INFO     Progress: 36%, Train RMSE Loss: 0.0755
2023-05-14 02:02:23,060 seq2seq.trainer.supervised_trainer INFO     Progress: 37%, Train RMSE Loss: 0.0755
2023-05-14 02:02:44,352 seq2seq.trainer.supervised_trainer INFO     Progress: 39%, Train RMSE Loss: 0.0757
2023-05-14 02:03:16,573 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 4: Train RMSE Loss: 0.0756, Dev RMSE Loss: 0.0751, Test RMSE Loss: 0.0758
2023-05-14 02:03:30,949 seq2seq.trainer.supervised_trainer INFO     Progress: 41%, Train RMSE Loss: 0.0755
2023-05-14 02:03:52,799 seq2seq.trainer.supervised_trainer INFO     Progress: 42%, Train RMSE Loss: 0.0756
2023-05-14 02:04:14,738 seq2seq.trainer.supervised_trainer INFO     Progress: 44%, Train RMSE Loss: 0.0754
2023-05-14 02:04:35,779 seq2seq.trainer.supervised_trainer INFO     Progress: 46%, Train RMSE Loss: 0.0750
2023-05-14 02:04:56,942 seq2seq.trainer.supervised_trainer INFO     Progress: 47%, Train RMSE Loss: 0.0756
2023-05-14 02:05:18,486 seq2seq.trainer.supervised_trainer INFO     Progress: 49%, Train RMSE Loss: 0.0752
2023-05-14 02:05:52,360 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 5: Train RMSE Loss: 0.0754, Dev RMSE Loss: 0.0744, Test RMSE Loss: 0.0750
2023-05-14 02:06:05,331 seq2seq.trainer.supervised_trainer INFO     Progress: 51%, Train RMSE Loss: 0.0761
2023-05-14 02:06:26,366 seq2seq.trainer.supervised_trainer INFO     Progress: 52%, Train RMSE Loss: 0.0761
2023-05-14 02:06:47,424 seq2seq.trainer.supervised_trainer INFO     Progress: 54%, Train RMSE Loss: 0.0754
2023-05-14 02:07:08,348 seq2seq.trainer.supervised_trainer INFO     Progress: 55%, Train RMSE Loss: 0.0753
2023-05-14 02:07:29,031 seq2seq.trainer.supervised_trainer INFO     Progress: 57%, Train RMSE Loss: 0.0755
2023-05-14 02:07:50,253 seq2seq.trainer.supervised_trainer INFO     Progress: 59%, Train RMSE Loss: 0.0752
2023-05-14 02:08:23,442 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 6: Train RMSE Loss: 0.0756, Dev RMSE Loss: 0.0742, Test RMSE Loss: 0.0748
2023-05-14 02:08:34,393 seq2seq.trainer.supervised_trainer INFO     Progress: 60%, Train RMSE Loss: 0.0753
2023-05-14 02:08:55,020 seq2seq.trainer.supervised_trainer INFO     Progress: 62%, Train RMSE Loss: 0.0754
2023-05-14 02:09:15,779 seq2seq.trainer.supervised_trainer INFO     Progress: 64%, Train RMSE Loss: 0.0753
2023-05-14 02:09:36,960 seq2seq.trainer.supervised_trainer INFO     Progress: 65%, Train RMSE Loss: 0.0751
2023-05-14 02:09:56,681 seq2seq.trainer.supervised_trainer INFO     Progress: 67%, Train RMSE Loss: 0.0754
2023-05-14 02:10:17,319 seq2seq.trainer.supervised_trainer INFO     Progress: 69%, Train RMSE Loss: 0.0754
2023-05-14 02:10:51,986 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 7: Train RMSE Loss: 0.0753, Dev RMSE Loss: 0.0746, Test RMSE Loss: 0.0752
2023-05-14 02:11:01,030 seq2seq.trainer.supervised_trainer INFO     Progress: 70%, Train RMSE Loss: 0.0752
2023-05-14 02:11:21,407 seq2seq.trainer.supervised_trainer INFO     Progress: 72%, Train RMSE Loss: 0.0753
2023-05-14 02:11:41,882 seq2seq.trainer.supervised_trainer INFO     Progress: 74%, Train RMSE Loss: 0.0752
2023-05-14 02:12:01,499 seq2seq.trainer.supervised_trainer INFO     Progress: 75%, Train RMSE Loss: 0.0749
2023-05-14 02:12:21,548 seq2seq.trainer.supervised_trainer INFO     Progress: 77%, Train RMSE Loss: 0.0754
2023-05-14 02:12:41,614 seq2seq.trainer.supervised_trainer INFO     Progress: 78%, Train RMSE Loss: 0.0755
2023-05-14 02:13:16,774 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 8: Train RMSE Loss: 0.0753, Dev RMSE Loss: 0.0744, Test RMSE Loss: 0.0750
2023-05-14 02:13:24,522 seq2seq.trainer.supervised_trainer INFO     Progress: 80%, Train RMSE Loss: 0.0753
2023-05-14 02:13:44,401 seq2seq.trainer.supervised_trainer INFO     Progress: 82%, Train RMSE Loss: 0.0752
2023-05-14 02:14:04,709 seq2seq.trainer.supervised_trainer INFO     Progress: 83%, Train RMSE Loss: 0.0752
2023-05-14 02:14:25,532 seq2seq.trainer.supervised_trainer INFO     Progress: 85%, Train RMSE Loss: 0.0749
2023-05-14 02:14:46,615 seq2seq.trainer.supervised_trainer INFO     Progress: 87%, Train RMSE Loss: 0.0756
2023-05-14 02:15:06,334 seq2seq.trainer.supervised_trainer INFO     Progress: 88%, Train RMSE Loss: 0.0754
2023-05-14 02:15:42,827 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 9: Train RMSE Loss: 0.0752, Dev RMSE Loss: 0.0739, Test RMSE Loss: 0.0745
2023-05-14 02:15:48,585 seq2seq.trainer.supervised_trainer INFO     Progress: 90%, Train RMSE Loss: 0.0749
2023-05-14 02:16:07,743 seq2seq.trainer.supervised_trainer INFO     Progress: 92%, Train RMSE Loss: 0.0753
2023-05-14 02:16:27,494 seq2seq.trainer.supervised_trainer INFO     Progress: 93%, Train RMSE Loss: 0.0750
2023-05-14 02:16:47,078 seq2seq.trainer.supervised_trainer INFO     Progress: 95%, Train RMSE Loss: 0.0749
2023-05-14 02:17:07,105 seq2seq.trainer.supervised_trainer INFO     Progress: 97%, Train RMSE Loss: 0.0751
2023-05-14 02:17:27,196 seq2seq.trainer.supervised_trainer INFO     Progress: 98%, Train RMSE Loss: 0.0753
2023-05-14 02:18:55,380 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 10: Train RMSE Loss: 0.0751, Dev RMSE Loss: 0.0740, Test RMSE Loss: 0.0747
Train RMSE Loss from Evaluator: 0.0743
------Hyper-parameters------
model_name: transformer, batch_size: 64, hidden_size: 32, learning_rate: 0.001, num_epochs: 10, num_layers: 3, dropout_rate: 0.1, teacher_forcing_ratio: 0.5
input_size: 128, kernel_size: 3, num_head: 2, use_adamw: False
use_sbert = False, use_sbert_seq = False, in_len: 4, out_len: 5
----------------------------
--------------Best Model:--------------
Epoch 9, Train RMSE Loss: 0.0741, Dev RMSE Loss: 0.0739, Test RMSE Loss: 0.0747
---------------------------------------
