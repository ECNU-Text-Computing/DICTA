2023-05-14 01:56:52,267 root         INFO     Namespace(data_path='dblp', expt_dir='./experiment/baseline_lstm', log_level='info', model='rnn')
2023-05-14 01:56:52,267 sentence_transformers.SentenceTransformer INFO     Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2023-05-14 01:56:52,585 sentence_transformers.SentenceTransformer INFO     Use pytorch device: cuda
2023-05-14 01:56:54,583 seq2seq.trainer.supervised_trainer INFO     Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
), Scheduler: None
2023-05-14 01:57:04,649 seq2seq.trainer.supervised_trainer INFO     Progress: 3%, Train RMSE Loss: 0.2626
2023-05-14 01:57:09,288 seq2seq.trainer.supervised_trainer INFO     Progress: 4%, Train RMSE Loss: 0.1306
2023-05-14 01:57:14,823 seq2seq.trainer.supervised_trainer INFO     Progress: 6%, Train RMSE Loss: 0.1320
2023-05-14 01:57:20,426 seq2seq.trainer.supervised_trainer INFO     Progress: 8%, Train RMSE Loss: 0.1311
2023-05-14 01:57:25,989 seq2seq.trainer.supervised_trainer INFO     Progress: 9%, Train RMSE Loss: 0.1309
2023-05-14 01:57:36,114 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 1: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:57:40,882 seq2seq.trainer.supervised_trainer INFO     Progress: 11%, Train RMSE Loss: 0.1313
2023-05-14 01:57:46,243 seq2seq.trainer.supervised_trainer INFO     Progress: 13%, Train RMSE Loss: 0.1314
2023-05-14 01:57:52,001 seq2seq.trainer.supervised_trainer INFO     Progress: 14%, Train RMSE Loss: 0.1306
2023-05-14 01:57:57,640 seq2seq.trainer.supervised_trainer INFO     Progress: 16%, Train RMSE Loss: 0.1319
2023-05-14 01:58:02,687 seq2seq.trainer.supervised_trainer INFO     Progress: 18%, Train RMSE Loss: 0.1311
2023-05-14 01:58:07,896 seq2seq.trainer.supervised_trainer INFO     Progress: 19%, Train RMSE Loss: 0.1310
2023-05-14 01:58:18,461 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 2: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:58:23,175 seq2seq.trainer.supervised_trainer INFO     Progress: 21%, Train RMSE Loss: 0.1312
2023-05-14 01:58:29,095 seq2seq.trainer.supervised_trainer INFO     Progress: 23%, Train RMSE Loss: 0.1313
2023-05-14 01:58:34,753 seq2seq.trainer.supervised_trainer INFO     Progress: 24%, Train RMSE Loss: 0.1308
2023-05-14 01:58:40,379 seq2seq.trainer.supervised_trainer INFO     Progress: 26%, Train RMSE Loss: 0.1317
2023-05-14 01:58:46,035 seq2seq.trainer.supervised_trainer INFO     Progress: 27%, Train RMSE Loss: 0.1311
2023-05-14 01:58:51,688 seq2seq.trainer.supervised_trainer INFO     Progress: 29%, Train RMSE Loss: 0.1309
2023-05-14 01:59:02,659 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 3: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:59:07,205 seq2seq.trainer.supervised_trainer INFO     Progress: 31%, Train RMSE Loss: 0.1313
2023-05-14 01:59:12,870 seq2seq.trainer.supervised_trainer INFO     Progress: 32%, Train RMSE Loss: 0.1316
2023-05-14 01:59:18,557 seq2seq.trainer.supervised_trainer INFO     Progress: 34%, Train RMSE Loss: 0.1309
2023-05-14 01:59:24,217 seq2seq.trainer.supervised_trainer INFO     Progress: 36%, Train RMSE Loss: 0.1314
2023-05-14 01:59:29,827 seq2seq.trainer.supervised_trainer INFO     Progress: 37%, Train RMSE Loss: 0.1312
2023-05-14 01:59:35,169 seq2seq.trainer.supervised_trainer INFO     Progress: 39%, Train RMSE Loss: 0.1311
2023-05-14 01:59:45,774 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 4: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 01:59:49,598 seq2seq.trainer.supervised_trainer INFO     Progress: 41%, Train RMSE Loss: 0.1310
2023-05-14 01:59:54,604 seq2seq.trainer.supervised_trainer INFO     Progress: 42%, Train RMSE Loss: 0.1317
2023-05-14 02:00:00,250 seq2seq.trainer.supervised_trainer INFO     Progress: 44%, Train RMSE Loss: 0.1309
2023-05-14 02:00:05,872 seq2seq.trainer.supervised_trainer INFO     Progress: 46%, Train RMSE Loss: 0.1314
2023-05-14 02:00:11,245 seq2seq.trainer.supervised_trainer INFO     Progress: 47%, Train RMSE Loss: 0.1309
2023-05-14 02:00:16,923 seq2seq.trainer.supervised_trainer INFO     Progress: 49%, Train RMSE Loss: 0.1314
2023-05-14 02:00:29,062 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 5: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 02:00:32,718 seq2seq.trainer.supervised_trainer INFO     Progress: 51%, Train RMSE Loss: 0.1310
2023-05-14 02:00:38,339 seq2seq.trainer.supervised_trainer INFO     Progress: 52%, Train RMSE Loss: 0.1318
2023-05-14 02:00:43,874 seq2seq.trainer.supervised_trainer INFO     Progress: 54%, Train RMSE Loss: 0.1309
2023-05-14 02:00:49,276 seq2seq.trainer.supervised_trainer INFO     Progress: 55%, Train RMSE Loss: 0.1312
2023-05-14 02:00:54,977 seq2seq.trainer.supervised_trainer INFO     Progress: 57%, Train RMSE Loss: 0.1311
2023-05-14 02:01:00,597 seq2seq.trainer.supervised_trainer INFO     Progress: 59%, Train RMSE Loss: 0.1315
2023-05-14 02:01:13,172 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 6: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 02:01:16,365 seq2seq.trainer.supervised_trainer INFO     Progress: 60%, Train RMSE Loss: 0.1304
2023-05-14 02:01:21,880 seq2seq.trainer.supervised_trainer INFO     Progress: 62%, Train RMSE Loss: 0.1324
2023-05-14 02:01:26,880 seq2seq.trainer.supervised_trainer INFO     Progress: 64%, Train RMSE Loss: 0.1303
2023-05-14 02:01:31,747 seq2seq.trainer.supervised_trainer INFO     Progress: 65%, Train RMSE Loss: 0.1312
2023-05-14 02:01:36,793 seq2seq.trainer.supervised_trainer INFO     Progress: 67%, Train RMSE Loss: 0.1312
2023-05-14 02:01:42,433 seq2seq.trainer.supervised_trainer INFO     Progress: 69%, Train RMSE Loss: 0.1318
2023-05-14 02:01:55,315 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 7: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 02:01:57,945 seq2seq.trainer.supervised_trainer INFO     Progress: 70%, Train RMSE Loss: 0.1304
2023-05-14 02:02:03,318 seq2seq.trainer.supervised_trainer INFO     Progress: 72%, Train RMSE Loss: 0.1321
2023-05-14 02:02:08,418 seq2seq.trainer.supervised_trainer INFO     Progress: 74%, Train RMSE Loss: 0.1305
2023-05-14 02:02:14,025 seq2seq.trainer.supervised_trainer INFO     Progress: 75%, Train RMSE Loss: 0.1313
2023-05-14 02:02:19,643 seq2seq.trainer.supervised_trainer INFO     Progress: 77%, Train RMSE Loss: 0.1311
2023-05-14 02:02:25,267 seq2seq.trainer.supervised_trainer INFO     Progress: 78%, Train RMSE Loss: 0.1321
2023-05-14 02:02:38,376 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 8: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 02:02:40,779 seq2seq.trainer.supervised_trainer INFO     Progress: 80%, Train RMSE Loss: 0.1306
2023-05-14 02:02:46,416 seq2seq.trainer.supervised_trainer INFO     Progress: 82%, Train RMSE Loss: 0.1316
2023-05-14 02:02:52,038 seq2seq.trainer.supervised_trainer INFO     Progress: 83%, Train RMSE Loss: 0.1306
2023-05-14 02:02:57,692 seq2seq.trainer.supervised_trainer INFO     Progress: 85%, Train RMSE Loss: 0.1315
2023-05-14 02:03:03,321 seq2seq.trainer.supervised_trainer INFO     Progress: 87%, Train RMSE Loss: 0.1307
2023-05-14 02:03:08,887 seq2seq.trainer.supervised_trainer INFO     Progress: 88%, Train RMSE Loss: 0.1321
2023-05-14 02:03:21,202 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 9: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
2023-05-14 02:03:22,752 seq2seq.trainer.supervised_trainer INFO     Progress: 90%, Train RMSE Loss: 0.1308
2023-05-14 02:03:27,809 seq2seq.trainer.supervised_trainer INFO     Progress: 92%, Train RMSE Loss: 0.1317
2023-05-14 02:03:33,259 seq2seq.trainer.supervised_trainer INFO     Progress: 93%, Train RMSE Loss: 0.1304
2023-05-14 02:03:38,780 seq2seq.trainer.supervised_trainer INFO     Progress: 95%, Train RMSE Loss: 0.1311
2023-05-14 02:03:44,304 seq2seq.trainer.supervised_trainer INFO     Progress: 97%, Train RMSE Loss: 0.1314
2023-05-14 02:03:50,037 seq2seq.trainer.supervised_trainer INFO     Progress: 98%, Train RMSE Loss: 0.1319
2023-05-14 02:04:21,627 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 10: Train RMSE Loss: 0.1313, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
Train RMSE Loss from Evaluator: 0.1312
------Hyper-parameters------
model_name: rnn, batch_size: 64, hidden_size: 32, learning_rate: 0.001, num_epochs: 10, num_layers: 3, dropout_rate: 0.1, teacher_forcing_ratio: 1
input_size: 128, kernel_size: 3, num_head: 2, use_adamw: False
use_sbert = False, use_sbert_seq = False, in_len: 4, out_len: 5
----------------------------
--------------Best Model:--------------
Epoch 1, Train RMSE Loss: 0.1312, Dev RMSE Loss: 0.1305, Test RMSE Loss: 0.1318
---------------------------------------
