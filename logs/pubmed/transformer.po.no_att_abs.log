2023-05-14 00:14:04,665 root         INFO     Namespace(data_path='po', expt_dir='./experiment/no_att_abs', log_level='info', model='transformer')
2023-05-14 00:14:04,665 sentence_transformers.SentenceTransformer INFO     Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2023-05-14 00:14:05,037 sentence_transformers.SentenceTransformer INFO     Use pytorch device: cuda
2023-05-14 00:14:07,308 seq2seq.trainer.supervised_trainer INFO     Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
), Scheduler: None
2023-05-14 00:14:31,391 seq2seq.trainer.supervised_trainer INFO     Progress: 5%, Train RMSE Loss: 0.1267
2023-05-14 00:14:43,031 seq2seq.trainer.supervised_trainer INFO     Progress: 8%, Train RMSE Loss: 0.0595
2023-05-14 00:15:00,510 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 1: Train RMSE Loss: 0.0618, Dev RMSE Loss: 0.0601, Test RMSE Loss: 0.0597
2023-05-14 00:15:08,012 seq2seq.trainer.supervised_trainer INFO     Progress: 11%, Train RMSE Loss: 0.0589
2023-05-14 00:15:19,659 seq2seq.trainer.supervised_trainer INFO     Progress: 14%, Train RMSE Loss: 0.0585
2023-05-14 00:15:31,794 seq2seq.trainer.supervised_trainer INFO     Progress: 17%, Train RMSE Loss: 0.0585
2023-05-14 00:15:53,028 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 2: Train RMSE Loss: 0.0586, Dev RMSE Loss: 0.0570, Test RMSE Loss: 0.0566
2023-05-14 00:15:56,616 seq2seq.trainer.supervised_trainer INFO     Progress: 20%, Train RMSE Loss: 0.0584
2023-05-14 00:16:08,512 seq2seq.trainer.supervised_trainer INFO     Progress: 23%, Train RMSE Loss: 0.0582
2023-05-14 00:16:19,994 seq2seq.trainer.supervised_trainer INFO     Progress: 26%, Train RMSE Loss: 0.0580
2023-05-14 00:16:32,207 seq2seq.trainer.supervised_trainer INFO     Progress: 29%, Train RMSE Loss: 0.0576
2023-05-14 00:16:46,285 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 3: Train RMSE Loss: 0.0581, Dev RMSE Loss: 0.0573, Test RMSE Loss: 0.0570
2023-05-14 00:16:57,046 seq2seq.trainer.supervised_trainer INFO     Progress: 32%, Train RMSE Loss: 0.0582
2023-05-14 00:17:08,312 seq2seq.trainer.supervised_trainer INFO     Progress: 35%, Train RMSE Loss: 0.0579
2023-05-14 00:17:19,896 seq2seq.trainer.supervised_trainer INFO     Progress: 38%, Train RMSE Loss: 0.0576
2023-05-14 00:17:37,768 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 4: Train RMSE Loss: 0.0579, Dev RMSE Loss: 0.0600, Test RMSE Loss: 0.0595
2023-05-14 00:17:44,502 seq2seq.trainer.supervised_trainer INFO     Progress: 41%, Train RMSE Loss: 0.0580
2023-05-14 00:17:56,516 seq2seq.trainer.supervised_trainer INFO     Progress: 44%, Train RMSE Loss: 0.0576
2023-05-14 00:18:07,948 seq2seq.trainer.supervised_trainer INFO     Progress: 47%, Train RMSE Loss: 0.0571
2023-05-14 00:18:29,874 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 5: Train RMSE Loss: 0.0575, Dev RMSE Loss: 0.0577, Test RMSE Loss: 0.0573
2023-05-14 00:18:32,803 seq2seq.trainer.supervised_trainer INFO     Progress: 50%, Train RMSE Loss: 0.0577
2023-05-14 00:18:45,444 seq2seq.trainer.supervised_trainer INFO     Progress: 53%, Train RMSE Loss: 0.0576
2023-05-14 00:18:57,315 seq2seq.trainer.supervised_trainer INFO     Progress: 56%, Train RMSE Loss: 0.0575
2023-05-14 00:19:08,760 seq2seq.trainer.supervised_trainer INFO     Progress: 59%, Train RMSE Loss: 0.0573
2023-05-14 00:19:23,422 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 6: Train RMSE Loss: 0.0576, Dev RMSE Loss: 0.0572, Test RMSE Loss: 0.0568
2023-05-14 00:19:33,549 seq2seq.trainer.supervised_trainer INFO     Progress: 62%, Train RMSE Loss: 0.0575
2023-05-14 00:19:45,808 seq2seq.trainer.supervised_trainer INFO     Progress: 65%, Train RMSE Loss: 0.0571
2023-05-14 00:19:57,757 seq2seq.trainer.supervised_trainer INFO     Progress: 68%, Train RMSE Loss: 0.0573
2023-05-14 00:20:16,935 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 7: Train RMSE Loss: 0.0573, Dev RMSE Loss: 0.0565, Test RMSE Loss: 0.0561
2023-05-14 00:20:22,941 seq2seq.trainer.supervised_trainer INFO     Progress: 71%, Train RMSE Loss: 0.0576
2023-05-14 00:20:34,393 seq2seq.trainer.supervised_trainer INFO     Progress: 74%, Train RMSE Loss: 0.0571
2023-05-14 00:20:46,048 seq2seq.trainer.supervised_trainer INFO     Progress: 77%, Train RMSE Loss: 0.0571
2023-05-14 00:21:09,021 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 8: Train RMSE Loss: 0.0572, Dev RMSE Loss: 0.0568, Test RMSE Loss: 0.0564
2023-05-14 00:21:10,862 seq2seq.trainer.supervised_trainer INFO     Progress: 80%, Train RMSE Loss: 0.0569
2023-05-14 00:21:22,634 seq2seq.trainer.supervised_trainer INFO     Progress: 83%, Train RMSE Loss: 0.0571
2023-05-14 00:21:34,343 seq2seq.trainer.supervised_trainer INFO     Progress: 86%, Train RMSE Loss: 0.0567
2023-05-14 00:21:46,146 seq2seq.trainer.supervised_trainer INFO     Progress: 89%, Train RMSE Loss: 0.0571
2023-05-14 00:22:01,941 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 9: Train RMSE Loss: 0.0571, Dev RMSE Loss: 0.0561, Test RMSE Loss: 0.0558
2023-05-14 00:22:11,156 seq2seq.trainer.supervised_trainer INFO     Progress: 92%, Train RMSE Loss: 0.0571
2023-05-14 00:22:22,715 seq2seq.trainer.supervised_trainer INFO     Progress: 95%, Train RMSE Loss: 0.0570
2023-05-14 00:22:34,557 seq2seq.trainer.supervised_trainer INFO     Progress: 98%, Train RMSE Loss: 0.0568
2023-05-14 00:23:23,648 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 10: Train RMSE Loss: 0.0570, Dev RMSE Loss: 0.0570, Test RMSE Loss: 0.0565
Train RMSE Loss from Evaluator: 0.0564
------Hyper-parameters------
model_name: transformer, batch_size: 64, hidden_size: 32, learning_rate: 0.001, num_epochs: 10, num_layers: 3, dropout_rate: 0.1, teacher_forcing_ratio: 1
input_size: 128, kernel_size: 3, num_head: 2, use_adamw: False
use_sbert = False, use_sbert_seq = False, in_len: 5, out_len: 5
----------------------------
--------------Best Model:--------------
Epoch 9, Train RMSE Loss: 0.0557, Dev RMSE Loss: 0.0561, Test RMSE Loss: 0.0565
---------------------------------------
