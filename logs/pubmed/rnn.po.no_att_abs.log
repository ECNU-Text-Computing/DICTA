2023-05-14 00:01:08,359 root         INFO     Namespace(data_path='po', expt_dir='./experiment/no_att_abs', log_level='info', model='rnn')
2023-05-14 00:01:08,359 sentence_transformers.SentenceTransformer INFO     Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2023-05-14 00:01:09,153 sentence_transformers.SentenceTransformer INFO     Use pytorch device: cuda
2023-05-14 00:01:11,028 seq2seq.trainer.supervised_trainer INFO     Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
), Scheduler: None
2023-05-14 00:01:22,436 seq2seq.trainer.supervised_trainer INFO     Progress: 5%, Train RMSE Loss: 0.0674
2023-05-14 00:01:27,599 seq2seq.trainer.supervised_trainer INFO     Progress: 8%, Train RMSE Loss: 0.0273
2023-05-14 00:01:34,713 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 1: Train RMSE Loss: 0.0311, Dev RMSE Loss: 0.1578, Test RMSE Loss: 0.1580
2023-05-14 00:01:39,220 seq2seq.trainer.supervised_trainer INFO     Progress: 11%, Train RMSE Loss: 0.0262
2023-05-14 00:01:45,849 seq2seq.trainer.supervised_trainer INFO     Progress: 14%, Train RMSE Loss: 0.0264
2023-05-14 00:01:52,132 seq2seq.trainer.supervised_trainer INFO     Progress: 17%, Train RMSE Loss: 0.0246
2023-05-14 00:02:02,898 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 2: Train RMSE Loss: 0.0253, Dev RMSE Loss: 0.1404, Test RMSE Loss: 0.1406
2023-05-14 00:02:05,212 seq2seq.trainer.supervised_trainer INFO     Progress: 20%, Train RMSE Loss: 0.0242
2023-05-14 00:02:11,906 seq2seq.trainer.supervised_trainer INFO     Progress: 23%, Train RMSE Loss: 0.0245
2023-05-14 00:02:18,521 seq2seq.trainer.supervised_trainer INFO     Progress: 26%, Train RMSE Loss: 0.0229
2023-05-14 00:02:25,113 seq2seq.trainer.supervised_trainer INFO     Progress: 29%, Train RMSE Loss: 0.0221
2023-05-14 00:02:31,090 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 3: Train RMSE Loss: 0.0233, Dev RMSE Loss: 0.1295, Test RMSE Loss: 0.1297
2023-05-14 00:02:37,207 seq2seq.trainer.supervised_trainer INFO     Progress: 32%, Train RMSE Loss: 0.0211
2023-05-14 00:02:43,431 seq2seq.trainer.supervised_trainer INFO     Progress: 35%, Train RMSE Loss: 0.0211
2023-05-14 00:02:49,634 seq2seq.trainer.supervised_trainer INFO     Progress: 38%, Train RMSE Loss: 0.0204
2023-05-14 00:02:57,546 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 4: Train RMSE Loss: 0.0209, Dev RMSE Loss: 0.1108, Test RMSE Loss: 0.1109
2023-05-14 00:03:01,148 seq2seq.trainer.supervised_trainer INFO     Progress: 41%, Train RMSE Loss: 0.0203
2023-05-14 00:03:07,180 seq2seq.trainer.supervised_trainer INFO     Progress: 44%, Train RMSE Loss: 0.0193
2023-05-14 00:03:13,663 seq2seq.trainer.supervised_trainer INFO     Progress: 47%, Train RMSE Loss: 0.0187
2023-05-14 00:03:24,119 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 5: Train RMSE Loss: 0.0192, Dev RMSE Loss: 0.1050, Test RMSE Loss: 0.1051
2023-05-14 00:03:25,750 seq2seq.trainer.supervised_trainer INFO     Progress: 50%, Train RMSE Loss: 0.0191
2023-05-14 00:03:32,127 seq2seq.trainer.supervised_trainer INFO     Progress: 53%, Train RMSE Loss: 0.0192
2023-05-14 00:03:38,436 seq2seq.trainer.supervised_trainer INFO     Progress: 56%, Train RMSE Loss: 0.0185
2023-05-14 00:03:44,641 seq2seq.trainer.supervised_trainer INFO     Progress: 59%, Train RMSE Loss: 0.0178
2023-05-14 00:03:50,332 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 6: Train RMSE Loss: 0.0185, Dev RMSE Loss: 0.1117, Test RMSE Loss: 0.1119
2023-05-14 00:03:56,312 seq2seq.trainer.supervised_trainer INFO     Progress: 62%, Train RMSE Loss: 0.0177
2023-05-14 00:04:02,946 seq2seq.trainer.supervised_trainer INFO     Progress: 65%, Train RMSE Loss: 0.0181
2023-05-14 00:04:09,534 seq2seq.trainer.supervised_trainer INFO     Progress: 68%, Train RMSE Loss: 0.0187
2023-05-14 00:04:18,308 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 7: Train RMSE Loss: 0.0182, Dev RMSE Loss: 0.0971, Test RMSE Loss: 0.0972
2023-05-14 00:04:21,708 seq2seq.trainer.supervised_trainer INFO     Progress: 71%, Train RMSE Loss: 0.0184
2023-05-14 00:04:28,038 seq2seq.trainer.supervised_trainer INFO     Progress: 74%, Train RMSE Loss: 0.0176
2023-05-14 00:04:34,096 seq2seq.trainer.supervised_trainer INFO     Progress: 77%, Train RMSE Loss: 0.0175
2023-05-14 00:04:45,006 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 8: Train RMSE Loss: 0.0177, Dev RMSE Loss: 0.0958, Test RMSE Loss: 0.0959
2023-05-14 00:04:46,122 seq2seq.trainer.supervised_trainer INFO     Progress: 80%, Train RMSE Loss: 0.0176
2023-05-14 00:04:52,481 seq2seq.trainer.supervised_trainer INFO     Progress: 83%, Train RMSE Loss: 0.0177
2023-05-14 00:04:58,814 seq2seq.trainer.supervised_trainer INFO     Progress: 86%, Train RMSE Loss: 0.0173
2023-05-14 00:05:05,152 seq2seq.trainer.supervised_trainer INFO     Progress: 89%, Train RMSE Loss: 0.0171
2023-05-14 00:05:11,773 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 9: Train RMSE Loss: 0.0174, Dev RMSE Loss: 0.1040, Test RMSE Loss: 0.1041
2023-05-14 00:05:16,849 seq2seq.trainer.supervised_trainer INFO     Progress: 92%, Train RMSE Loss: 0.0169
2023-05-14 00:05:23,323 seq2seq.trainer.supervised_trainer INFO     Progress: 95%, Train RMSE Loss: 0.0164
2023-05-14 00:05:31,041 seq2seq.trainer.supervised_trainer INFO     Progress: 98%, Train RMSE Loss: 0.0175
2023-05-14 00:06:02,892 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 10: Train RMSE Loss: 0.0169, Dev RMSE Loss: 0.0978, Test RMSE Loss: 0.0980
Train RMSE Loss from Evaluator: 0.0978
------Hyper-parameters------
model_name: rnn, batch_size: 64, hidden_size: 32, learning_rate: 0.001, num_epochs: 10, num_layers: 3, dropout_rate: 0.1, teacher_forcing_ratio: 0.5
input_size: 128, kernel_size: 3, num_head: 2, use_adamw: False
use_sbert = False, use_sbert_seq = False, in_len: 5, out_len: 5
----------------------------
--------------Best Model:--------------
Epoch 8, Train RMSE Loss: 0.0958, Dev RMSE Loss: 0.0958, Test RMSE Loss: 0.0980
---------------------------------------
