2023-05-14 01:55:41,467 root         INFO     Namespace(data_path='pubmed', expt_dir='./experiment/baseline_rnn', log_level='info', model='rnn')
2023-05-14 01:55:41,467 sentence_transformers.SentenceTransformer INFO     Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2023-05-14 01:55:41,829 sentence_transformers.SentenceTransformer INFO     Use pytorch device: cuda
2023-05-14 01:55:44,729 seq2seq.trainer.supervised_trainer INFO     Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
), Scheduler: None
2023-05-14 01:56:02,267 seq2seq.trainer.supervised_trainer INFO     Progress: 3%, Train RMSE Loss: 0.0252
2023-05-14 01:56:10,409 seq2seq.trainer.supervised_trainer INFO     Progress: 5%, Train RMSE Loss: 0.0088
2023-05-14 01:56:18,365 seq2seq.trainer.supervised_trainer INFO     Progress: 7%, Train RMSE Loss: 0.0080
2023-05-14 01:56:26,328 seq2seq.trainer.supervised_trainer INFO     Progress: 9%, Train RMSE Loss: 0.0078
2023-05-14 01:56:40,234 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 1: Train RMSE Loss: 0.0100, Dev RMSE Loss: 0.1906, Test RMSE Loss: 0.1898
2023-05-14 01:56:48,628 seq2seq.trainer.supervised_trainer INFO     Progress: 11%, Train RMSE Loss: 0.0077
2023-05-14 01:56:56,701 seq2seq.trainer.supervised_trainer INFO     Progress: 13%, Train RMSE Loss: 0.0074
2023-05-14 01:57:04,938 seq2seq.trainer.supervised_trainer INFO     Progress: 15%, Train RMSE Loss: 0.0070
2023-05-14 01:57:13,519 seq2seq.trainer.supervised_trainer INFO     Progress: 17%, Train RMSE Loss: 0.0072
2023-05-14 01:57:21,556 seq2seq.trainer.supervised_trainer INFO     Progress: 19%, Train RMSE Loss: 0.0070
2023-05-14 01:57:34,934 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 2: Train RMSE Loss: 0.0073, Dev RMSE Loss: 0.1906, Test RMSE Loss: 0.1898
2023-05-14 01:57:43,412 seq2seq.trainer.supervised_trainer INFO     Progress: 21%, Train RMSE Loss: 0.0069
2023-05-14 01:57:51,662 seq2seq.trainer.supervised_trainer INFO     Progress: 23%, Train RMSE Loss: 0.0070
2023-05-14 01:57:59,693 seq2seq.trainer.supervised_trainer INFO     Progress: 25%, Train RMSE Loss: 0.0063
2023-05-14 01:58:07,911 seq2seq.trainer.supervised_trainer INFO     Progress: 27%, Train RMSE Loss: 0.0066
2023-05-14 01:58:16,128 seq2seq.trainer.supervised_trainer INFO     Progress: 29%, Train RMSE Loss: 0.0065
2023-05-14 01:58:29,531 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 3: Train RMSE Loss: 0.0066, Dev RMSE Loss: 0.1906, Test RMSE Loss: 0.1898
2023-05-14 01:58:37,730 seq2seq.trainer.supervised_trainer INFO     Progress: 31%, Train RMSE Loss: 0.0062
2023-05-14 01:58:45,888 seq2seq.trainer.supervised_trainer INFO     Progress: 33%, Train RMSE Loss: 0.0062
2023-05-14 01:58:53,894 seq2seq.trainer.supervised_trainer INFO     Progress: 35%, Train RMSE Loss: 0.0061
2023-05-14 01:59:02,006 seq2seq.trainer.supervised_trainer INFO     Progress: 37%, Train RMSE Loss: 0.0060
2023-05-14 01:59:10,010 seq2seq.trainer.supervised_trainer INFO     Progress: 39%, Train RMSE Loss: 0.0058
2023-05-14 01:59:23,718 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 4: Train RMSE Loss: 0.0061, Dev RMSE Loss: 0.1906, Test RMSE Loss: 0.1898
2023-05-14 01:59:31,778 seq2seq.trainer.supervised_trainer INFO     Progress: 41%, Train RMSE Loss: 0.0060
2023-05-14 01:59:39,908 seq2seq.trainer.supervised_trainer INFO     Progress: 43%, Train RMSE Loss: 0.0058
2023-05-14 01:59:48,123 seq2seq.trainer.supervised_trainer INFO     Progress: 45%, Train RMSE Loss: 0.0056
2023-05-14 01:59:56,380 seq2seq.trainer.supervised_trainer INFO     Progress: 47%, Train RMSE Loss: 0.0056
2023-05-14 02:00:04,574 seq2seq.trainer.supervised_trainer INFO     Progress: 49%, Train RMSE Loss: 0.0056
2023-05-14 02:00:18,515 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 5: Train RMSE Loss: 0.0057, Dev RMSE Loss: 0.1906, Test RMSE Loss: 0.1898
2023-05-14 02:00:26,562 seq2seq.trainer.supervised_trainer INFO     Progress: 51%, Train RMSE Loss: 0.0053
2023-05-14 02:00:34,255 seq2seq.trainer.supervised_trainer INFO     Progress: 53%, Train RMSE Loss: 0.0051
2023-05-14 02:00:41,895 seq2seq.trainer.supervised_trainer INFO     Progress: 55%, Train RMSE Loss: 0.0053
2023-05-14 02:00:49,982 seq2seq.trainer.supervised_trainer INFO     Progress: 57%, Train RMSE Loss: 0.0050
2023-05-14 02:00:58,212 seq2seq.trainer.supervised_trainer INFO     Progress: 59%, Train RMSE Loss: 0.0048
2023-05-14 02:01:11,728 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 6: Train RMSE Loss: 0.0051, Dev RMSE Loss: 0.1906, Test RMSE Loss: 0.1898
2023-05-14 02:01:19,885 seq2seq.trainer.supervised_trainer INFO     Progress: 61%, Train RMSE Loss: 0.0048
2023-05-14 02:01:28,299 seq2seq.trainer.supervised_trainer INFO     Progress: 63%, Train RMSE Loss: 0.0045
2023-05-14 02:01:36,429 seq2seq.trainer.supervised_trainer INFO     Progress: 65%, Train RMSE Loss: 0.0044
2023-05-14 02:01:44,484 seq2seq.trainer.supervised_trainer INFO     Progress: 67%, Train RMSE Loss: 0.0048
2023-05-14 02:01:52,586 seq2seq.trainer.supervised_trainer INFO     Progress: 69%, Train RMSE Loss: 0.0043
2023-05-14 02:02:07,049 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 7: Train RMSE Loss: 0.0046, Dev RMSE Loss: 0.1906, Test RMSE Loss: 0.1898
2023-05-14 02:02:15,228 seq2seq.trainer.supervised_trainer INFO     Progress: 71%, Train RMSE Loss: 0.0045
2023-05-14 02:02:23,502 seq2seq.trainer.supervised_trainer INFO     Progress: 73%, Train RMSE Loss: 0.0045
2023-05-14 02:02:31,728 seq2seq.trainer.supervised_trainer INFO     Progress: 75%, Train RMSE Loss: 0.0044
2023-05-14 02:02:40,146 seq2seq.trainer.supervised_trainer INFO     Progress: 77%, Train RMSE Loss: 0.0042
2023-05-14 02:02:48,135 seq2seq.trainer.supervised_trainer INFO     Progress: 79%, Train RMSE Loss: 0.0043
2023-05-14 02:03:01,236 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 8: Train RMSE Loss: 0.0044, Dev RMSE Loss: 0.1905, Test RMSE Loss: 0.1898
2023-05-14 02:03:08,850 seq2seq.trainer.supervised_trainer INFO     Progress: 81%, Train RMSE Loss: 0.0041
2023-05-14 02:03:16,745 seq2seq.trainer.supervised_trainer INFO     Progress: 83%, Train RMSE Loss: 0.0041
2023-05-14 02:03:25,017 seq2seq.trainer.supervised_trainer INFO     Progress: 85%, Train RMSE Loss: 0.0040
2023-05-14 02:03:33,237 seq2seq.trainer.supervised_trainer INFO     Progress: 87%, Train RMSE Loss: 0.0042
2023-05-14 02:03:41,457 seq2seq.trainer.supervised_trainer INFO     Progress: 89%, Train RMSE Loss: 0.0041
2023-05-14 02:03:55,288 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 9: Train RMSE Loss: 0.0041, Dev RMSE Loss: 0.1905, Test RMSE Loss: 0.1897
2023-05-14 02:04:03,067 seq2seq.trainer.supervised_trainer INFO     Progress: 91%, Train RMSE Loss: 0.0042
2023-05-14 02:04:11,328 seq2seq.trainer.supervised_trainer INFO     Progress: 93%, Train RMSE Loss: 0.0042
2023-05-14 02:04:19,458 seq2seq.trainer.supervised_trainer INFO     Progress: 95%, Train RMSE Loss: 0.0040
2023-05-14 02:04:27,916 seq2seq.trainer.supervised_trainer INFO     Progress: 97%, Train RMSE Loss: 0.0040
2023-05-14 02:04:36,223 seq2seq.trainer.supervised_trainer INFO     Progress: 99%, Train RMSE Loss: 0.0040
2023-05-14 02:05:20,469 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 10: Train RMSE Loss: 0.0041, Dev RMSE Loss: 0.1905, Test RMSE Loss: 0.1898
Train RMSE Loss from Evaluator: 0.1899
------Hyper-parameters------
model_name: rnn, batch_size: 64, hidden_size: 32, learning_rate: 0.001, num_epochs: 10, num_layers: 3, dropout_rate: 0.1, teacher_forcing_ratio: 1
input_size: 128, kernel_size: 3, num_head: 2, use_adamw: False
use_sbert = False, use_sbert_seq = False, in_len: 4, out_len: 5
----------------------------
--------------Best Model:--------------
Epoch 9, Train RMSE Loss: 0.1899, Dev RMSE Loss: 0.1905, Test RMSE Loss: 0.1898
---------------------------------------
