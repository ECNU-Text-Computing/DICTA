2023-05-21 18:46:49,199 root         INFO     Namespace(data_path='do', expt_dir='./experiment/scibert', log_level='info', model='transformer')
2023-05-21 18:46:49,200 sentence_transformers.SentenceTransformer INFO     Load pretrained SentenceTransformer: all-MiniLM-L6-v2
2023-05-21 18:46:49,571 sentence_transformers.SentenceTransformer INFO     Use pytorch device: cuda
Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
2023-05-21 18:46:56,602 seq2seq.trainer.supervised_trainer INFO     Optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.001
    maximize: False
    weight_decay: 0
), Scheduler: None
2023-05-21 19:05:11,147 seq2seq.trainer.supervised_trainer INFO     Progress: 3%, Train RMSE Loss: 0.0802
2023-05-21 19:14:16,669 seq2seq.trainer.supervised_trainer INFO     Progress: 4%, Train RMSE Loss: 0.0327
2023-05-21 19:23:26,410 seq2seq.trainer.supervised_trainer INFO     Progress: 6%, Train RMSE Loss: 0.0321
2023-05-21 19:32:35,103 seq2seq.trainer.supervised_trainer INFO     Progress: 8%, Train RMSE Loss: 0.0325
2023-05-21 19:41:48,696 seq2seq.trainer.supervised_trainer INFO     Progress: 9%, Train RMSE Loss: 0.0323
2023-05-21 19:50:46,440 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 1: Train RMSE Loss: 0.0350, Dev RMSE Loss: 0.0360, Test RMSE Loss: 0.0358
2023-05-21 19:59:12,947 seq2seq.trainer.supervised_trainer INFO     Progress: 11%, Train RMSE Loss: 0.0322
2023-05-21 20:08:17,029 seq2seq.trainer.supervised_trainer INFO     Progress: 13%, Train RMSE Loss: 0.0317
2023-05-21 20:17:25,747 seq2seq.trainer.supervised_trainer INFO     Progress: 14%, Train RMSE Loss: 0.0312
2023-05-21 20:26:32,502 seq2seq.trainer.supervised_trainer INFO     Progress: 16%, Train RMSE Loss: 0.0306
2023-05-21 20:35:43,157 seq2seq.trainer.supervised_trainer INFO     Progress: 18%, Train RMSE Loss: 0.0302
2023-05-21 20:44:54,136 seq2seq.trainer.supervised_trainer INFO     Progress: 19%, Train RMSE Loss: 0.0303
2023-05-21 20:54:37,512 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 2: Train RMSE Loss: 0.0309, Dev RMSE Loss: 0.0268, Test RMSE Loss: 0.0267
2023-05-21 21:02:18,660 seq2seq.trainer.supervised_trainer INFO     Progress: 21%, Train RMSE Loss: 0.0296
2023-05-21 21:11:27,550 seq2seq.trainer.supervised_trainer INFO     Progress: 23%, Train RMSE Loss: 0.0298
2023-05-21 21:21:09,051 seq2seq.trainer.supervised_trainer INFO     Progress: 24%, Train RMSE Loss: 0.0297
2023-05-21 21:30:17,341 seq2seq.trainer.supervised_trainer INFO     Progress: 26%, Train RMSE Loss: 0.0296
2023-05-21 21:39:25,968 seq2seq.trainer.supervised_trainer INFO     Progress: 27%, Train RMSE Loss: 0.0294
2023-05-21 21:48:37,234 seq2seq.trainer.supervised_trainer INFO     Progress: 29%, Train RMSE Loss: 0.0292
2023-05-21 21:59:05,043 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 3: Train RMSE Loss: 0.0296, Dev RMSE Loss: 0.0276, Test RMSE Loss: 0.0275
2023-05-21 22:06:03,106 seq2seq.trainer.supervised_trainer INFO     Progress: 31%, Train RMSE Loss: 0.0291
2023-05-21 22:15:08,674 seq2seq.trainer.supervised_trainer INFO     Progress: 32%, Train RMSE Loss: 0.0289
2023-05-21 22:24:14,734 seq2seq.trainer.supervised_trainer INFO     Progress: 34%, Train RMSE Loss: 0.0291
2023-05-21 22:33:20,931 seq2seq.trainer.supervised_trainer INFO     Progress: 36%, Train RMSE Loss: 0.0289
2023-05-21 22:42:30,145 seq2seq.trainer.supervised_trainer INFO     Progress: 37%, Train RMSE Loss: 0.0288
2023-05-21 22:51:44,265 seq2seq.trainer.supervised_trainer INFO     Progress: 39%, Train RMSE Loss: 0.0287
2023-05-21 23:02:53,608 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 4: Train RMSE Loss: 0.0290, Dev RMSE Loss: 0.0273, Test RMSE Loss: 0.0271
2023-05-21 23:09:07,189 seq2seq.trainer.supervised_trainer INFO     Progress: 41%, Train RMSE Loss: 0.0289
2023-05-21 23:18:13,980 seq2seq.trainer.supervised_trainer INFO     Progress: 42%, Train RMSE Loss: 0.0286
2023-05-21 23:27:19,769 seq2seq.trainer.supervised_trainer INFO     Progress: 44%, Train RMSE Loss: 0.0291
2023-05-21 23:36:24,530 seq2seq.trainer.supervised_trainer INFO     Progress: 46%, Train RMSE Loss: 0.0286
2023-05-21 23:45:34,557 seq2seq.trainer.supervised_trainer INFO     Progress: 47%, Train RMSE Loss: 0.0286
2023-05-21 23:54:49,232 seq2seq.trainer.supervised_trainer INFO     Progress: 49%, Train RMSE Loss: 0.0286
2023-05-22 00:06:40,946 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 5: Train RMSE Loss: 0.0287, Dev RMSE Loss: 0.0283, Test RMSE Loss: 0.0282
2023-05-22 00:12:11,170 seq2seq.trainer.supervised_trainer INFO     Progress: 51%, Train RMSE Loss: 0.0285
2023-05-22 00:21:17,982 seq2seq.trainer.supervised_trainer INFO     Progress: 52%, Train RMSE Loss: 0.0284
2023-05-22 00:30:23,844 seq2seq.trainer.supervised_trainer INFO     Progress: 54%, Train RMSE Loss: 0.0284
2023-05-22 00:39:27,926 seq2seq.trainer.supervised_trainer INFO     Progress: 55%, Train RMSE Loss: 0.0284
2023-05-22 00:48:39,236 seq2seq.trainer.supervised_trainer INFO     Progress: 57%, Train RMSE Loss: 0.0281
2023-05-22 00:57:52,362 seq2seq.trainer.supervised_trainer INFO     Progress: 59%, Train RMSE Loss: 0.0282
2023-05-22 01:10:29,026 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 6: Train RMSE Loss: 0.0283, Dev RMSE Loss: 0.0278, Test RMSE Loss: 0.0277
2023-05-22 01:15:15,608 seq2seq.trainer.supervised_trainer INFO     Progress: 60%, Train RMSE Loss: 0.0281
2023-05-22 01:24:22,614 seq2seq.trainer.supervised_trainer INFO     Progress: 62%, Train RMSE Loss: 0.0282
2023-05-22 01:33:30,001 seq2seq.trainer.supervised_trainer INFO     Progress: 64%, Train RMSE Loss: 0.0282
2023-05-22 01:42:32,828 seq2seq.trainer.supervised_trainer INFO     Progress: 65%, Train RMSE Loss: 0.0284
2023-05-22 01:51:43,380 seq2seq.trainer.supervised_trainer INFO     Progress: 67%, Train RMSE Loss: 0.0280
2023-05-22 02:00:55,983 seq2seq.trainer.supervised_trainer INFO     Progress: 69%, Train RMSE Loss: 0.0281
2023-05-22 02:14:16,368 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 7: Train RMSE Loss: 0.0282, Dev RMSE Loss: 0.0272, Test RMSE Loss: 0.0271
2023-05-22 02:18:18,186 seq2seq.trainer.supervised_trainer INFO     Progress: 70%, Train RMSE Loss: 0.0280
2023-05-22 02:27:25,128 seq2seq.trainer.supervised_trainer INFO     Progress: 72%, Train RMSE Loss: 0.0281
2023-05-22 02:36:30,824 seq2seq.trainer.supervised_trainer INFO     Progress: 74%, Train RMSE Loss: 0.0280
2023-05-22 02:45:35,085 seq2seq.trainer.supervised_trainer INFO     Progress: 75%, Train RMSE Loss: 0.0281
2023-05-22 02:54:44,814 seq2seq.trainer.supervised_trainer INFO     Progress: 77%, Train RMSE Loss: 0.0279
2023-05-22 03:04:00,020 seq2seq.trainer.supervised_trainer INFO     Progress: 78%, Train RMSE Loss: 0.0277
2023-05-22 03:18:00,118 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 8: Train RMSE Loss: 0.0280, Dev RMSE Loss: 0.0273, Test RMSE Loss: 0.0273
2023-05-22 03:21:17,341 seq2seq.trainer.supervised_trainer INFO     Progress: 80%, Train RMSE Loss: 0.0278
2023-05-22 03:30:24,430 seq2seq.trainer.supervised_trainer INFO     Progress: 82%, Train RMSE Loss: 0.0279
2023-05-22 03:39:30,180 seq2seq.trainer.supervised_trainer INFO     Progress: 83%, Train RMSE Loss: 0.0279
2023-05-22 03:48:33,203 seq2seq.trainer.supervised_trainer INFO     Progress: 85%, Train RMSE Loss: 0.0279
2023-05-22 03:57:41,775 seq2seq.trainer.supervised_trainer INFO     Progress: 87%, Train RMSE Loss: 0.0278
2023-05-22 04:06:53,638 seq2seq.trainer.supervised_trainer INFO     Progress: 88%, Train RMSE Loss: 0.0280
2023-05-22 04:21:38,053 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 9: Train RMSE Loss: 0.0279, Dev RMSE Loss: 0.0263, Test RMSE Loss: 0.0263
2023-05-22 04:24:11,407 seq2seq.trainer.supervised_trainer INFO     Progress: 90%, Train RMSE Loss: 0.0275
2023-05-22 04:33:19,889 seq2seq.trainer.supervised_trainer INFO     Progress: 92%, Train RMSE Loss: 0.0278
2023-05-22 04:42:24,458 seq2seq.trainer.supervised_trainer INFO     Progress: 93%, Train RMSE Loss: 0.0279
2023-05-22 04:51:28,972 seq2seq.trainer.supervised_trainer INFO     Progress: 95%, Train RMSE Loss: 0.0281
2023-05-22 05:00:34,630 seq2seq.trainer.supervised_trainer INFO     Progress: 97%, Train RMSE Loss: 0.0276
2023-05-22 05:09:46,181 seq2seq.trainer.supervised_trainer INFO     Progress: 98%, Train RMSE Loss: 0.0280
2023-05-22 05:44:24,369 seq2seq.trainer.supervised_trainer INFO     
Finished epoch 10: Train RMSE Loss: 0.0278, Dev RMSE Loss: 0.0273, Test RMSE Loss: 0.0273
Train RMSE Loss from Evaluator: 0.0275
------Hyper-parameters------
model_name: transformer, batch_size: 64, hidden_size: 32, learning_rate: 0.001, num_epochs: 10, num_layers: 3, dropout_rate: 0.1, teacher_forcing_ratio: 0.5
input_size: 128, kernel_size: 3, num_head: 2, use_adamw: False
use_sbert = True, use_sbert_seq = True, in_len: 5, out_len: 5
----------------------------
--------------Best Model:--------------
Epoch 9, Train RMSE Loss: 0.0264, Dev RMSE Loss: 0.0263, Test RMSE Loss: 0.0273
---------------------------------------
